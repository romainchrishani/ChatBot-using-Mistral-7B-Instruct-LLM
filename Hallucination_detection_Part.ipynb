{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!pip install spacy\n",
        "!pip install bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR15sIam2e6l",
        "outputId": "d4432d9f-d95c-4f01-8459-9df94677a8a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m847.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (42.0.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.1 pypdfium2-4.30.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.3.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.0.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.41.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->bert_score)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.23.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n",
            "Successfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "from difflib import SequenceMatcher\n",
        "from bert_score import score as bert_score"
      ],
      "metadata": {
        "id": "vJMkE87c1zLL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqOaMdjy1wgT",
        "outputId": "c663b7d8-f290-4470-fa4d-e2c6d146e9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load spaCy model for Named Entity Recognition (NER)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from each page of a PDF and cleans it.\"\"\"\n",
        "    pages_text = []\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    cleaned_text = clean_text(text)\n",
        "                    table_sentences = extract_rows_from_table(cleaned_text)\n",
        "                    pages_text.append((i + 1, table_sentences))\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF: {e}\")\n",
        "    return pages_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Cleans the input text for easier processing.\"\"\"\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'\\s{2,}', '. ', text)\n",
        "    text = re.sub(r'(\\d)([A-Za-z])', r'\\1. \\2', text)\n",
        "    text = re.sub(r'\\.(\\w)', r'. \\1', text)\n",
        "    text = re.sub(r'(\\w)([A-Z])', r'\\1. \\2', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "LQ0WNSXG2OV-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_rows_from_table(table_text):\n",
        "    \"\"\"Extracts sentences from cleaned text.\"\"\"\n",
        "    rows = table_text.split('. ')\n",
        "    sentences = []\n",
        "    for row in rows:\n",
        "        columns = re.split(r'\\s{2,}', row)\n",
        "        if columns:\n",
        "            sentence = ' '.join(columns).strip()\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "LxuVdNGH2NAB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def tokenize_sentences(pages_text):\n",
        "    \"\"\"Tokenizes sentences from the pages text and keeps track of their positions.\"\"\"\n",
        "    sentences = []\n",
        "    sentence_positions = []\n",
        "    for page_num, page_sentences in pages_text:\n",
        "        for i, sentence in enumerate(page_sentences):\n",
        "            sentences.append(sentence)\n",
        "            sentence_positions.append((page_num, i))\n",
        "    return sentences, sentence_positions"
      ],
      "metadata": {
        "id": "y1ka4AMd2Lmu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def filter_important_words(sentence):\n",
        "    \"\"\"Filters out stop words and retains important words using spaCy.\"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    important_words = [\n",
        "        token.text.lower() for token in doc\n",
        "        if token.text.lower() not in stop_words and (token.ent_type_ or token.pos_ in {'NOUN', 'VERB', 'PROPN', 'NUM'})\n",
        "    ]\n",
        "    return ' '.join(important_words)"
      ],
      "metadata": {
        "id": "X0UetG0Q2KHG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def compute_lexical_similarity(reference_sentences, candidate_sentence):\n",
        "    \"\"\"Computes the lexical similarity between reference and candidate sentences.\"\"\"\n",
        "    filtered_candidate = filter_important_words(candidate_sentence)\n",
        "    filtered_references = [filter_important_words(sentence) for sentence in reference_sentences]\n",
        "    vectorizer = TfidfVectorizer().fit_transform(filtered_references + [filtered_candidate])\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_similarities = cosine_similarity(vectors)\n",
        "    return cosine_similarities[-1][:-1]"
      ],
      "metadata": {
        "id": "c7k3WiUt2IbA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def compute_bertscore_similarity(reference_sentences, candidate_sentence):\n",
        "    \"\"\"Computes the BERTScore similarity between reference and candidate sentences.\"\"\"\n",
        "    P, R, F1 = bert_score([candidate_sentence] * len(reference_sentences), reference_sentences, lang='en', verbose=False)\n",
        "    return F1.numpy()"
      ],
      "metadata": {
        "id": "vJO_GjKt2GtK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def find_most_similar_sentence(reference_sentences, candidate_sentences, sentence_positions):\n",
        "    \"\"\"Finds the most similar sentences and computes the mean lexical and BERTScore similarity scores.\"\"\"\n",
        "    all_lexical_scores = []\n",
        "    all_bertscore_scores = []\n",
        "    most_similar_sentences = []\n",
        "    most_similar_positions = []\n",
        "\n",
        "    for candidate_sentence in candidate_sentences:\n",
        "        lexical_similarities = compute_lexical_similarity(reference_sentences, candidate_sentence)\n",
        "        most_similar_index_lexical = np.argmax(lexical_similarities)\n",
        "        most_similar_sentence_lexical = reference_sentences[most_similar_index_lexical]\n",
        "        lexical_score = lexical_similarities[most_similar_index_lexical]\n",
        "\n",
        "        bertscore_similarities = compute_bertscore_similarity(reference_sentences, candidate_sentence)\n",
        "        most_similar_index_bertscore = np.argmax(bertscore_similarities)\n",
        "        most_similar_sentence_bertscore = reference_sentences[most_similar_index_bertscore]\n",
        "        bertscore_score = bertscore_similarities[most_similar_index_bertscore]\n",
        "\n",
        "        all_lexical_scores.append(lexical_score)\n",
        "        all_bertscore_scores.append(bertscore_score)\n",
        "        most_similar_sentences.append((most_similar_sentence_lexical, most_similar_sentence_bertscore))\n",
        "        most_similar_positions.append(sentence_positions[most_similar_index_lexical])\n",
        "\n",
        "    mean_lexical_score = np.mean(all_lexical_scores)\n",
        "    mean_bertscore_score = np.mean(all_bertscore_scores)\n",
        "    return most_similar_sentences, mean_lexical_score, mean_bertscore_score, most_similar_positions"
      ],
      "metadata": {
        "id": "7utYARTI2EHD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_matching_sentences(candidate_sentences, most_similar_sentences, positions):\n",
        "    \"\"\"Prints the matching sentences nicely for each candidate sentence.\"\"\"\n",
        "    for i, candidate_sentence in enumerate(candidate_sentences):\n",
        "        lexical_sentence, bertscore_sentence = most_similar_sentences[i]\n",
        "        page_num, sentence_index = positions[i]\n",
        "        print(f\"Candidate Sentence {i+1}: {candidate_sentence}\")\n",
        "        print(f\"  Most Similar Sentence (Lexical Similarity): {lexical_sentence}\")\n",
        "        print(f\"  Most Similar Sentence (BERTScore): {bertscore_sentence}\")\n",
        "        print(f\"  Position in PDF: Page {page_num}, Sentence Index {sentence_index}\")\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "poC_ADsE5ttG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(pdf_path, candidate_text):\n",
        "    \"\"\"Main function to execute the PDF text extraction and similarity finding.\"\"\"\n",
        "    pages_text = extract_text_from_pdf(pdf_path)\n",
        "    reference_sentences, sentence_positions = tokenize_sentences(pages_text)\n",
        "    candidate_sentences = nltk.sent_tokenize(candidate_text)\n",
        "\n",
        "    most_similar_sentences, mean_lexical_score, mean_bertscore_score, positions = find_most_similar_sentence(reference_sentences, candidate_sentences, sentence_positions)\n",
        "\n",
        "    print_matching_sentences(candidate_sentences, most_similar_sentences, positions)\n",
        "\n",
        "    print(f\"Mean Lexical Similarity Score: {mean_lexical_score}\")\n",
        "    print(f\"Mean BERTScore Similarity Score: {mean_bertscore_score}\")\n",
        "\n",
        "# Example usage\n",
        "pdf_path = '/content/2023.pdf'\n",
        "candidate_text = \"\"\" When you enter from the main vehicle entrance, turn to your right. The multi -story building is the library.\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(pdf_path, candidate_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7esgW4wu5z22",
        "outputId": "c86744f8-f8af-4c01-b5d4-99e5420e840f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "\n",
        "def main(pdf_path, candidate_text):\n",
        "    \"\"\"Main function to execute the PDF text extraction and similarity finding.\"\"\"\n",
        "    pages_text = extract_text_from_pdf(pdf_path)\n",
        "    reference_sentences, sentence_positions = tokenize_sentences(pages_text)\n",
        "    candidate_sentences = nltk.sent_tokenize(candidate_text)\n",
        "\n",
        "    most_similar_sentences, mean_lexical_score, mean_bertscore_score, positions = find_most_similar_sentence(reference_sentences, candidate_sentences, sentence_positions)\n",
        "\n",
        "    print(f\"Most similar sentences (Lexical Similarity and BERTScore): {most_similar_sentences}\")\n",
        "    print(f\"Mean Lexical Similarity Score: {mean_lexical_score}\")\n",
        "    print(f\"Mean BERTScore Similarity Score: {mean_bertscore_score}\")\n",
        "    for i, (page_num, sentence_index) in enumerate(positions):\n",
        "        print(f\"Position in PDF for candidate sentence {i+1} - Page: {page_num}, Sentence Index: {sentence_index}\")\n",
        "\n",
        "        # Highlight differences\n",
        "        #changes = highlight_differences(candidate_sentences[i], most_similar_sentences[i][0])\n",
        "        #if changes:\n",
        "            #print(f\"Changes in candidate sentence {i+1} (Lexical): {changes}\")\n",
        "\n",
        "        #changes = highlight_differences(candidate_sentences[i], most_similar_sentences[i][1])\n",
        "        #if changes:\n",
        "            #print(f\"Changes in candidate sentence {i+1} (BERTScore): {changes}\")\n",
        "\n",
        "# Example usage\n",
        "pdf_path = '/content/2023.pdf'\n",
        "candidate_text = \"\"\"There are three departments in the faculty.\n",
        "\n",
        "14\n",
        "\n",
        "Message from the Head, Department of Computational Mathematics\n",
        "Welcome to the Faculty of Computational Mathematics, University of Moratuwa!\n",
        "On behalf of the Department of Computational Mathematics, I would like to\n",
        "warmly welcome you to the faculty. It is a great pleasure to see hundreds of\n",
        "determined and dedicated young adults entrusting their future with the Faculty\n",
        "of Computational Mathematics. As  you begin your academic career in this\n",
        "prestigious institution, we congratulate you on your achievement, and your\n",
        "insight in choosing a program with high demand in this rapidly evolving\n",
        "discipline.\n",
        "Department of Computational Mathematics remains one of the main academic\n",
        "departments providing the nation with professionally qualified mathematicians,\n",
        "scientists, and researchers in the fields of Computational Mathematics,\n",
        "Computer Science, and Information Technology. The curricula encompass a\n",
        "wide variety of subjects in Computational Mathematics, Computer Science,\n",
        "and Information Technology disciplines to provide both theoretical knowledge\n",
        "and practical exposure. Furthermore, the Department sets high emphasis on\n",
        "research studies and group work.\n",
        "The Department maintains an unwavering reputation for its contribution in\n",
        "presenting academically sound, competent, and high -quality graduates to\n",
        "the workforce in the fields of Computational Mathematics, Computer Science,\n",
        "and Information Technology. There are quite a considerable number of\n",
        "graduates securing higher studies opportunities and scholarships in top -ranking\n",
        "international universities, immediately after graduation. We were also\n",
        "fortunate to produce several IT entrepreneurs whose startup company has grown\n",
        "into highly reputed award -winning companies with international recognition.\n",
        "We, the Department of Computational Mathematics, encourage you to envision\n",
        "your future today, explore opportunities, embrace diversity, and be\n",
        "competent individuals with direction.\n",
        "Wish you all a memorable and inspiring stay at the University of Moratuwa!\n",
        "\n",
        "\n",
        "\n",
        "Mrs. Wijewardene\n",
        "Head, Department of Computational Mathematics\n",
        "Tel - office:  0112 -650894 ext.8200\n",
        "web\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(pdf_path, candidate_text)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV-lNb1b1-ne",
        "outputId": "c4177a17-bdd1-4231-fef3-4274396b26d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar sentences (Lexical Similarity and BERTScore): [('The three Departments are; • Department of Information Technology • Department of Computational Mathematics • Department of Interdisciplinary Studies Presently, the Departments collaboratively offer subjects for the three undergraduate degree programmes conducted by the faculty', 'There are four main canteens and six other sales centres in the University'), ('Department of Computational Mathematics The Department of Computational Mathematics is one of the key pillars of the Faculty of Information Technology', 'Message from the Head, Department of Information Technology Welcome to the Faculty of Information Technology, University of Moratuwa! On behalf of the Department of Information Technology, I would like to warmly welcome you to the faculty'), ('On behalf of the Department of Computational Mathematics, let me take the opportunity to warmly welcome you to the Faculty of Information Technology at the University of Moratuwa', 'On behalf of the Department of Computational Mathematics, let me take the opportunity to warmly welcome you to the Faculty of Information Technology at the University of Moratuwa'), ('It is a great pleasure to see hundreds of determined and dedicated young adults entrusting their future with the Faculty of Information Technology', 'It is a great pleasure to see hundreds of determined and dedicated young adults entrusting their future with the Faculty of Information Technology'), ('As you begin your academic career in this prestigious institution, we congratulate you on your achievement, and your insight in choosing a program with high demand in this rapidly evolving discipline', 'As you begin your academic career in this prestigious institution, we congratulate you on your achievement, and your insight in choosing a program with high demand in this rapidly evolving discipline'), ('Department of Computational Mathematics The Department of Computational Mathematics is one of the key pillars of the Faculty of Information Technology', 'Department of Information Technology remains one of the main academic departments providing the nation with professionally qualified computing practitioners, educators, and researchers'), ('The curricula encompass a wide variety of subjects in Computer Science/Information Technology disciplines to provide both theoretical knowledge and practical exposure', 'The curricula encompass a wide variety of subjects in Computer Science/Information Technology disciplines to provide both theoretical knowledge and practical exposure'), ('Furthermore, the Department sets high emphasis on research studies and group work', 'Furthermore, the Department sets high emphasis on research studies and group work'), ('The Department maintains an unwavering reputation for its contribution in presenting academically sound, competent, and high-quality I', 'The Department maintains an unwavering reputation for its contribution in presenting academically sound, competent, and high-quality I'), ('There are quite a considerable number of graduates securing higher studies opportunities and scholarships in top-ranking international universities, immediately after graduation', 'There are quite a considerable number of graduates securing higher studies opportunities and scholarships in top-ranking international universities, immediately after graduation'), ('T entrepreneurs whose startup company has grown into highly reputed award-winning companies with international recognition', 'T entrepreneurs whose startup company has grown into highly reputed award-winning companies with international recognition'), ('We, the Department of Information Technology, encourage you to envision your future today, explore opportunities, embrace diversity, and be competent individuals with direction', 'We, the Department of Information Technology, encourage you to envision your future today, explore opportunities, embrace diversity, and be competent individuals with direction'), ('Wish you all a memorable and inspiring stay at the University of Moratuwa! Mrs', 'Wish you all a memorable and inspiring stay at the University of Moratuwa! Mrs'), ('Karunaratne Head, Department of Information Technology Tel - office: 0112-650894 ext', 'Thushari Silva Head, Department of Computational Mathematics Tel - office: 0112-650893 ext')]\n",
            "Mean Lexical Similarity Score: 0.8022019672210098\n",
            "Mean BERTScore Similarity Score: 0.9268584251403809\n",
            "Position in PDF for candidate sentence 1 - Page: 18, Sentence Index: 14\n",
            "Position in PDF for candidate sentence 2 - Page: 23, Sentence Index: 0\n",
            "Position in PDF for candidate sentence 3 - Page: 11, Sentence Index: 12\n",
            "Position in PDF for candidate sentence 4 - Page: 9, Sentence Index: 1\n",
            "Position in PDF for candidate sentence 5 - Page: 9, Sentence Index: 2\n",
            "Position in PDF for candidate sentence 6 - Page: 23, Sentence Index: 0\n",
            "Position in PDF for candidate sentence 7 - Page: 9, Sentence Index: 5\n",
            "Position in PDF for candidate sentence 8 - Page: 9, Sentence Index: 6\n",
            "Position in PDF for candidate sentence 9 - Page: 9, Sentence Index: 7\n",
            "Position in PDF for candidate sentence 10 - Page: 9, Sentence Index: 10\n",
            "Position in PDF for candidate sentence 11 - Page: 9, Sentence Index: 12\n",
            "Position in PDF for candidate sentence 12 - Page: 9, Sentence Index: 13\n",
            "Position in PDF for candidate sentence 13 - Page: 9, Sentence Index: 14\n",
            "Position in PDF for candidate sentence 14 - Page: 9, Sentence Index: 18\n"
          ]
        }
      ]
    }
  ]
}